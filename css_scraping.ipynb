{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mocha\\AppData\\Local\\Temp\\ipykernel_24580\\1167413270.py:30: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for element in soup.find_all(text=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No span tags found with class 'select2-selection__placeholder'\n",
      "Scraped data saved to scraped_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from openpyxl import Workbook\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Set up Selenium WebDriver\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ChromeService(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load, adjust the time as necessary\n",
    "\n",
    "        # Initialize a list to store extracted text elements\n",
    "        text_elements = []\n",
    "\n",
    "        # Find all text elements\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for element in soup.find_all(text=True):\n",
    "            if element.parent.name not in [\n",
    "                \"style\",\n",
    "                \"script\",\n",
    "                \"head\",\n",
    "                \"title\",\n",
    "                \"meta\",\n",
    "                \"[document]\",\n",
    "            ]:\n",
    "                text_elements.append(element.strip())\n",
    "\n",
    "        # Get placeholder text from input tags\n",
    "        for input_tag in soup.find_all(\"input\"):\n",
    "            placeholder = input_tag.get(\"placeholder\")\n",
    "            if placeholder:\n",
    "                text_elements.append(placeholder)\n",
    "\n",
    "        # Find the span element with class 'select2-selection__placeholder'\n",
    "        span_tags = soup.find_all(\"span\", class_=\"select2-selection__placeholder\")\n",
    "        if span_tags:\n",
    "            for span_tag in span_tags:\n",
    "                text = span_tag.get_text(strip=True)\n",
    "                text_elements.append(text)\n",
    "        else:\n",
    "            print(\"No span tags found with class 'select2-selection__placeholder'\")\n",
    "\n",
    "        # Filter out empty and whitespace-only strings\n",
    "        text_list = [text for text in text_elements if text.strip()]\n",
    "\n",
    "        return text_list\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/login\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"scraped_data.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Data\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
