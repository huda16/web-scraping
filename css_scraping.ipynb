{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping using Selenium and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to beranda.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from openpyxl import Workbook\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Set up Selenium WebDriver\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ChromeService(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load, adjust the time as necessary\n",
    "\n",
    "        # Initialize a list to store extracted text elements\n",
    "        text_elements = []\n",
    "\n",
    "        # Find all text elements\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for element in soup.find_all(string=True):\n",
    "            if element.parent.name not in [\n",
    "                \"style\",\n",
    "                \"script\",\n",
    "                \"head\",\n",
    "                \"title\",\n",
    "                \"meta\",\n",
    "                \"[document]\",\n",
    "            ]:\n",
    "                text_elements.append(element.strip())\n",
    "\n",
    "        # Get placeholder text from input tags\n",
    "        for input_tag in soup.find_all(\"input\"):\n",
    "            placeholder = input_tag.get(\"placeholder\")\n",
    "            if placeholder:\n",
    "                text_elements.append(placeholder)\n",
    "\n",
    "        # Filter out empty and whitespace-only strings\n",
    "        text_list = [text for text in text_elements if text.strip()]\n",
    "\n",
    "        return text_list\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/beranda\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"beranda.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Beranda\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping using Selenium and BeautifulSoup (Updated Script with Login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to input_kirimanmu.xlsx\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from openpyxl import Workbook\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url_login, url, username, password):\n",
    "    # Set up Selenium WebDriver\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ChromeService(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Open the login page\n",
    "        driver.get(url_login)\n",
    "\n",
    "        # Wait for the login page to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Find the username and password input fields and log in\n",
    "        username_field = driver.find_element(\n",
    "            By.NAME, \"login_username\"\n",
    "        )  # Adjust selector as needed\n",
    "        password_field = driver.find_element(\n",
    "            By.NAME, \"login_password\"\n",
    "        )  # Adjust selector as needed\n",
    "        username_field.send_keys(username)\n",
    "        password_field.send_keys(password)\n",
    "        password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for the login to complete and the page to load\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Navigate to the desired page after login\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "        # Initialize a list to store extracted text elements\n",
    "        text_elements = []\n",
    "\n",
    "        # Find all text elements\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for element in soup.find_all(string=True):\n",
    "            if element.parent.name not in [\n",
    "                \"style\",\n",
    "                \"script\",\n",
    "                \"head\",\n",
    "                \"title\",\n",
    "                \"meta\",\n",
    "                \"[document]\",\n",
    "            ]:\n",
    "                text_elements.append(element.strip())\n",
    "\n",
    "        # Get placeholder text from input tags\n",
    "        for input_tag in soup.find_all(\"input\"):\n",
    "            placeholder = input_tag.get(\"placeholder\")\n",
    "            if placeholder:\n",
    "                text_elements.append(placeholder)\n",
    "\n",
    "        # Filter out empty and whitespace-only strings\n",
    "        text_list = [text for text in text_elements if text.strip()]\n",
    "\n",
    "        return text_list\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url_login = \"https://css.jne.co.id/login\"  # Replace with the URL of the login page\n",
    "    url = \"https://css.jne.co.id/pesanan/kirim\"  # Replace with the URL of the webpage you want to scrape\n",
    "    username = \"your_username\"  # Replace with your username\n",
    "    password = \"your_password\"  # Replace with your password\n",
    "    output_file = \"input_kirimanmu.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url_login, url, username, password)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Input Kirimanmu\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
