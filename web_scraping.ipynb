{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to scraped_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "# Function to scrape text from specific HTML tags on a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Specify the HTML tags to scrape from\n",
    "    tags_to_scrape = [\n",
    "        \"p\",\n",
    "        \"h1\",\n",
    "        \"h2\",\n",
    "        \"h3\",\n",
    "        \"h4\",\n",
    "        \"h5\",\n",
    "        \"h6\",\n",
    "        \"li\",\n",
    "    ]  # Example tags (you can add/remove as needed)\n",
    "\n",
    "    # Initialize list to store sentences\n",
    "    sentences = []\n",
    "\n",
    "    # Extract text from each specified tag\n",
    "    for tag in tags_to_scrape:\n",
    "        elements = soup.find_all(tag)\n",
    "        for element in elements:\n",
    "            # Split text into sentences and add to sentences list\n",
    "            sentences.extend(element.get_text().strip().split(\". \"))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"scraped_data.xlsx\"\n",
    "\n",
    "    # Scrape the webpage\n",
    "    scraped_sentences = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Data\"\n",
    "\n",
    "    # Write each sentence to a new cell in column A\n",
    "    for idx, sentence in enumerate(scraped_sentences, start=1):\n",
    "        ws.cell(row=idx, column=1, value=sentence)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to scraped_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all text elements\n",
    "    text_elements = soup.find_all(string=True)\n",
    "\n",
    "    # Get placeholder text from input tags\n",
    "    for input_tag in soup.find_all(\"input\"):\n",
    "        placeholder = input_tag.get(\"placeholder\")\n",
    "        if placeholder:\n",
    "            text_elements.append(placeholder)\n",
    "\n",
    "    # Find the span element with class 'select2-selection__placeholder'\n",
    "    for span_tag in soup.find_all(\"span\", class_=\"select2-selection__placeholder\"):\n",
    "        if span_tag:\n",
    "            # Extract text from the span tag and strip whitespace\n",
    "            text = span_tag.get_text(strip=True)\n",
    "            text_elements.append(text)\n",
    "\n",
    "    # Filter out empty and whitespace-only strings\n",
    "    text_list = [text.strip() for text in text_elements if text.strip()]\n",
    "\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"scraped_data.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Data\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No span tags found with class 'select2-selection__placeholder'\n",
      "Scraped data saved to scraped_data.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mocha\\AppData\\Local\\Temp\\ipykernel_20908\\2110898523.py:17: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for element in soup.find_all(text=True):\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Initialize a list to store extracted text elements\n",
    "    text_elements = []\n",
    "\n",
    "    # Find all text elements\n",
    "    for element in soup.find_all(text=True):\n",
    "        if element.parent.name not in [\n",
    "            \"style\",\n",
    "            \"script\",\n",
    "            \"head\",\n",
    "            \"title\",\n",
    "            \"meta\",\n",
    "            \"[document]\",\n",
    "        ]:\n",
    "            text_elements.append(element.strip())\n",
    "\n",
    "    # Get placeholder text from input tags\n",
    "    for input_tag in soup.find_all(\"input\"):\n",
    "        placeholder = input_tag.get(\"placeholder\")\n",
    "        if placeholder:\n",
    "            text_elements.append(placeholder)\n",
    "\n",
    "    # Find the span element with class 'select2-selection__placeholder'\n",
    "    span_tags = soup.find_all(\"span\", class_=\"select2-selection__placeholder\")\n",
    "    if span_tags:\n",
    "        for span_tag in span_tags:\n",
    "            # Extract text from the span tag and strip whitespace\n",
    "            text = span_tag.get_text(strip=True)\n",
    "            text_elements.append(text)\n",
    "    else:\n",
    "        print(\"No span tags found with class 'select2-selection__placeholder'\")\n",
    "\n",
    "    # Filter out empty and whitespace-only strings\n",
    "    text_list = [text for text in text_elements if text.strip()]\n",
    "\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"scraped_data.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Data\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from openpyxl import Workbook\n",
    "\n",
    "\n",
    "# Function to scrape all text from a webpage\n",
    "def scrape_webpage(url):\n",
    "    # Set up Selenium WebDriver\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ChromeService(ChromeDriverManager().install()), options=options\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load, adjust the time as necessary\n",
    "\n",
    "        # Initialize a list to store extracted text elements\n",
    "        text_elements = []\n",
    "\n",
    "        # Find all text elements\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for element in soup.find_all(text=True):\n",
    "            if element.parent.name not in [\n",
    "                \"style\",\n",
    "                \"script\",\n",
    "                \"head\",\n",
    "                \"title\",\n",
    "                \"meta\",\n",
    "                \"[document]\",\n",
    "            ]:\n",
    "                text_elements.append(element.strip())\n",
    "\n",
    "        # Get placeholder text from input tags\n",
    "        for input_tag in soup.find_all(\"input\"):\n",
    "            placeholder = input_tag.get(\"placeholder\")\n",
    "            if placeholder:\n",
    "                text_elements.append(placeholder)\n",
    "\n",
    "        # Find the span element with class 'select2-selection__placeholder'\n",
    "        span_tags = soup.find_all(\"span\", class_=\"select2-selection__placeholder\")\n",
    "        if span_tags:\n",
    "            for span_tag in span_tags:\n",
    "                text = span_tag.get_text(strip=True)\n",
    "                text_elements.append(text)\n",
    "        else:\n",
    "            print(\"No span tags found with class 'select2-selection__placeholder'\")\n",
    "\n",
    "        # Filter out empty and whitespace-only strings\n",
    "        text_list = [text for text in text_elements if text.strip()]\n",
    "\n",
    "        return text_list\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    url = \"https://css.jne.co.id/\"  # Replace with the URL of the webpage you want to scrape\n",
    "    output_file = \"scraped_data.xlsx\"\n",
    "\n",
    "    # Scrape the webpage to get all text content\n",
    "    scraped_text = scrape_webpage(url)\n",
    "\n",
    "    # Write to Excel file\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Scraped Data\"\n",
    "\n",
    "    # Write each distinct piece of text to a new cell in column A\n",
    "    for idx, text in enumerate(scraped_text, start=1):\n",
    "        ws.cell(row=idx, column=1, value=text)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(output_file)\n",
    "    print(f\"Scraped data saved to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
